--set ambari.hive.db.schema.name=hive;
set fs.file.impl.disable.cache=true;
set fs.hdfs.impl.disable.cache=true;
set hive.auto.convert.join.noconditionaltask=true;
set hive.auto.convert.join=true;
set hive.auto.convert.sortmerge.join=true;
set hive.compactor.abortedtxn.threshold=1000;
set hive.compactor.check.interval=300;
set hive.compactor.delta.num.threshold=10;
set hive.compactor.delta.pct.threshold=0.1f;
set hive.compactor.initiator.on=false;
set hive.compactor.worker.threads=0;
set hive.compactor.worker.timeout=86400;
set hive.compute.query.using.stats=true;
set hive.enforce.bucketing=true;
set hive.enforce.sorting=true;
set hive.enforce.sortmergebucketmapjoin=true;

set hive.limit.pushdown.memory.usage=0.04;
set hive.map.aggr=true;
set hive.mapjoin.bucket.cache.size=10000;
set hive.mapred.reduce.tasks.speculative.execution=false;
set hive.metastore.cache.pinobjtypes=Table,Database,Type,FieldSchema,Order;
set hive.metastore.client.socket.timeout=60;
set hive.metastore.execute.setugi=true;
set hive.metastore.warehouse.dir=/apps/hive/warehouse;
set hive.optimize.bucketmapjoin.sortedmerge=false;
set hive.optimize.bucketmapjoin=true;
set hive.optimize.index.filter=true;
set hive.optimize.reducededuplication.min.reducer=4;
set hive.optimize.reducededuplication=true;
set hive.orc.splits.include.file.footer=false;
--set hive.security.authorization.enabled=false;
--set hive.security.metastore.authorization.manager=org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider;
set hive.server2.enable.doAs=false;
--set hive.server2.tez.default.queues=default;
--set hive.server2.tez.initialize.default.sessions=false;
--set hive.server2.tez.sessions.per.default.queue=1;
set hive.stats.autogather=true;
--set hive.tez.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
set hive.txn.max.open.batch=1000;
set hive.txn.timeout=300;

--set hive.vectorized.execution.enabled=true;
--set hive.vectorized.groupby.checkinterval=1024;
--set hive.vectorized.groupby.flush.percent=1;
--set hive.vectorized.groupby.maxentries=1024;

-- These values need to be tuned appropriately to your cluster. These examples are for reference.
-- set hive.tez.container.size=4096;
-- set hive.tez.java.opts=-Xmx3800m;
-- set hive.auto.convert.join.noconditionaltask.size=320000000;

-- set hive.execution.engine=mr;
-- set hive.default.fileformat=orc;
set hive.cbo.enable=true;
-- set hive.stats.fetch.column.stats=true;
set hive.exec.dynamic.partition.mode=nonstrict;
-- set hive.tez.auto.reducer.parallelism=true;
set hive.exec.reducers.bytes.per.reducer=100000000;
--set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager;
set hive.support.concurrency=false;
--set hive.tez.exec.print.summary=true;

set hive.strict.checks.cartesian.product=false;
set hive.mapred.mode=nonstrict;

-- ###########################
-- baseline options
-- ###########################
set hive.execution.engine=spark;
set hive.default.fileformat=sequencefile;

set hive.vectorized.execution.enabled=true;
set hive.vectorized.execution.reduce.enabled=true;
set hive.vectorized.use.row.serde.deserialize=false;
set hive.vectorized.use.vector.serde.deserialize=true;
set hive.vectorized.execution.reduce.groupby.enabled=true;

set spark.home=/opt/Beaver/spark;
set hive.spark.job.monitor.timeout=3600s;
set hive.spark.client.server.connect.timeout=3600s;
set hive.spark.client.connect.timeout=3600s;

set spark.yarn.report.interval=5000;
set spark.kryo.referenceTracking=false;
set spark.io.compression.codec=lzf;
set spark.executor.extraJavaOptions=-XX:+UseParallelOldGC -XX:ParallelGCThreads=4 -XX:NewRatio=1 -XX:SurvivorRatio=1;
set spark.memory.storageFraction=0.01;
set spark.driver.memory=4g;

set spark.driver.extraLibraryPath=/opt/Beaver/hadoop/lib/native;
set spark.executor.extraLibraryPath=/opt/Beaver/hadoop/lib/native;
set spark.yarn.am.extraLibraryPath=/opt/Beaver/hadoop/lib/native;

-- PARQUET compression options: UNCOMPRESSED,GZIP,SNAPPY
set parquet.compression=GZIP;

-- set parquet.block.size=32M
set parquet.block.size=33554432;

